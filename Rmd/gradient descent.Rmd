---
title: "Gradient_descend"
author: "Federico Cortese"
date: "15/7/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an example of usage of the functions "linear_gd_optim" and "linear_gd_optim2". The first function contains the analitycal expression for the gradient of the loss function, whereas the second makes use of the function "grad" from the package "numDeriv".
We except the first function to be faster than the second one.
Consider the following simulated dataset:

```{r}
library(gradescCM)
set.seed(8675309)
n = 1000
x1 = rnorm(n)
x2 = rnorm(n)
y = 1 + .5*x1 + .2*x2 + rnorm(n)
X=cbind(x1,x2)
options(warn=-1)
```

We set the initial values for the parameters' vector beta equal to


```{r}
b_pre=c(0,0,0)
tol=1e-6
maxit=10^5

```

For the first function we have

```{r}
linear_gd_optim(b_pre,X,y,tol=tol,maxit=maxit)

```

For the second function
```{r}
linear_gd_optim2(b_pre,X,y,tol=tol,maxit=maxit)

```


Time consuming: 
```{r}
library(dplyr)
library(purrr)
library(readr)
library(readxl)
library(ggplot2)
library(tidyr)


show_bm=function(bm){
  print(bm)
  autoplot(bm)
}

bench::mark(
  check=F,
  bare=NULL,
  first=linear_gd_optim(b_pre,X,y),
  second=linear_gd_optim2(b_pre,X,y)
)%>% show_bm()

```

The first version is clearly faster then the second one. 
Consider now the Steepest descend method

```{r}
basic_sd(b_pre,X,y,tol=tol,maxit=maxit)

```

Let us compare the Gradient Descend and the Steepest Descend:

```{r}
bench::mark(
  check=F,
  bare=NULL,
  GD=linear_gd_optim(b_pre,X,y),
  SD=basic_sd(b_pre,X,y)
)%>% show_bm()

```

